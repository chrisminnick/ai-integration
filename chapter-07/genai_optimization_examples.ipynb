{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da3be7e",
   "metadata": {},
   "source": [
    "# Generative AI Optimization Examples\n",
    "This notebook demonstrates several prompt optimization techniques using the OpenAI API. You'll need an API key to run these examples.\n",
    "\n",
    "> ⚠️ Make sure you have installed the `openai` Python package and set your `OPENAI_API_KEY` environment variable or configure it manually in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ad1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Optionally set your API key here if not using environment variables\n",
    "# openai.api_key = 'your-api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12375e88",
   "metadata": {},
   "source": [
    "## Example 1: Prompt Compression\n",
    "Reducing unnecessary instructions from the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1ec79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing the latency of your GPT-4 application in production can be achieved through several strategies:\n",
      "\n",
      "1. **Optimize your model**: Try using a smaller version of the GPT-4 model if the latency issue arises from the model's complexity. Smaller models can still deliver high performance with lower latency.\n",
      "\n",
      "2. **Parallel Processing**: Use parallel processing to divide the model's tasks across multiple CPUs, GPUs, or machines. This can dramatically reduce the time it takes to process requests.\n",
      "\n",
      "3. **Optimize your code**: Review your code for any inefficiencies or bottlenecks that could be slowing down processing. Use profiling tools to identify these problem areas.\n",
      "\n",
      "4. **Use Caching**: If your application often produces the same responses, caching these responses can reduce the need for the model to generate them, thereby reducing latency.\n",
      "\n",
      "5. **Upgrade your hardware**: Faster CPUs, more memory, and high-performance GPUs can all contribute to lower latency. Cloud-based solutions can provide scalable, on-demand access to such resources.\n",
      "\n",
      "6. **Batching**: If you have multiple simultaneous requests, process them in a single batch rather than individually. This can make better use of your hardware and reduce latency.\n",
      "\n",
      "7. **Asynchronous Processing**: If appropriate, use asynchronous processing so that a slow request does not block other requests.\n",
      "\n",
      "Remember to monitor your application's performance regularly, so you can identify and address any new sources of latency as they emerge.\n",
      "\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 52\n",
      "Completion tokens: 289\n",
      "Total tokens: 341\n"
     ]
    }
   ],
   "source": [
    "# Verbose prompt\n",
    "verbose_prompt = '''\n",
    "You're an intelligent and helpful assistant. Please help me answer this question in a clear, concise, and professional manner.\n",
    "The question is: How can I reduce the latency of my GPT-4 application in production?\n",
    "'''\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-4\",\n",
    "messages=[{\"role\": \"user\", \"content\": verbose_prompt}],\n",
    "temperature=0.7)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f600c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of the time of writing this, GPT-4 has not been released by OpenAI. Therefore, it's hard to provide specific advice on how to reduce its latency in production. However, the following general tips might be useful for optimizing AI models like GPT-3 and potentially GPT-4:\n",
      "\n",
      "1. Model Optimization: Use techniques like quantization, pruning, and knowledge distillation to reduce the complexity of the model. This could potentially reduce the time it takes to make predictions.\n",
      "\n",
      "2. Hardware Acceleration: Use GPUs or other hardware accelerators, which are often designed to efficiently run AI workloads.\n",
      "\n",
      "3. Efficient Coding: Optimize your code to reduce unnecessary computations. Make sure your implementation is as efficient as possible.\n",
      "\n",
      "4. Use Edge Computing: If possible, deploy the model closer to the user to reduce network latency. This could mean using edge computing solutions.\n",
      "\n",
      "5. Parallel Processing: If the model needs to make multiple predictions, try to parallelize these predictions to speed up the overall process.\n",
      "\n",
      "6. Use Efficient Data Structures: The way you store and access your data can have a big impact on performance. Make sure you're using the most efficient data structures for your specific use case.\n",
      "\n",
      "7. Model Pipelining: If the model is part of a larger pipeline, make sure the entire pipeline is as efficient as possible. This could mean optimizing the way data is passed between different stages of the pipeline.\n",
      "\n",
      "8. Caching: If the model needs to make the same prediction multiple times, consider caching the result to avoid unnecessary computation.\n",
      "\n",
      "Remember, it's always important to measure the performance of your model and the system it's running on to identify bottlenecks and validate whether your optimizations are having the desired effect.\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 19\n",
      "Completion tokens: 350\n",
      "Total tokens: 369\n"
     ]
    }
   ],
   "source": [
    "# Compressed prompt\n",
    "compressed_prompt = \"How can I reduce GPT-4 latency in production?\"\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-4\",\n",
    "messages=[{\"role\": \"user\", \"content\": compressed_prompt}],\n",
    "temperature=0.7)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1a4fa",
   "metadata": {},
   "source": [
    "## Example 2: Output Compression\n",
    "Limit the size and verbosity of the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e0db0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which plants, algae, and certain bacteria convert light energy from the sun into chemical energy stored in glucose molecules. This process is essential for the survival of plants and other photosynthetic organisms, as it provides them with the energy they need to grow and sustain life.\n",
      "\n",
      "Photosynthesis takes place in the chloroplasts of plant cells, where chlorophyll – a green pigment that absorbs light – is located. During photosynthesis, carbon dioxide from the air and water from the soil are taken up by the plant. The chlorophyll absorbs light energy from the sun and uses it to convert these raw materials into glucose and oxygen.\n",
      "\n",
      "The overall chemical equation for photosynthesis is:\n",
      "\n",
      "6CO2 + 6H2O + light energy → C6H12O6 + 6O2\n",
      "\n",
      "In this process, carbon dioxide is converted into glucose, a form of sugar that plants use as a source of energy. Oxygen is also produced as a byproduct and released into the atmosphere.\n",
      "\n",
      "Photosynthesis is a vital process for maintaining the balance of oxygen and carbon dioxide in the Earth's atmosphere. It also serves as the foundation of the food chain, as plants are the primary producers that other organisms rely on for sustenance.\n",
      "\n",
      "Overall, photosynthesis is a complex and intricate process that plays a crucial role in the survival of plants and the entire ecosystem.\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 13\n",
      "Completion tokens: 275\n",
      "Total tokens: 288\n"
     ]
    }
   ],
   "source": [
    "# Unconstrained response\n",
    "prompt = \"Tell me about photosynthesis.\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f860abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy into chemical energy.\n",
      "- This process involves using carbon dioxide, water, and sunlight to produce glucose (a sugar) and oxygen as a byproduct.\n",
      "- Photosynthesis is essential for the survival of plants and ultimately fuels most life on Earth by providing food and oxygen.\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 25\n",
      "Completion tokens: 72\n",
      "Total tokens: 97\n"
     ]
    }
   ],
   "source": [
    "# Constrained response\n",
    "prompt = \"Tell me about photosynthesis.\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Answer in 3 short bullet points.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "],\n",
    "max_tokens=100)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b00ba",
   "metadata": {},
   "source": [
    "## Example 3: Cache Prompts\n",
    "Return responses for duplicate prompts from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2571eb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:\n",
      "Cache miss. Calling API...\n",
      "Answer: The capital of France is Paris.\n",
      "\n",
      "Second call (should use cache):\n",
      "Cache hit! Returning cached response.\n",
      "Answer: The capital of France is Paris.\n",
      "\n",
      "Cache size: 1 entries\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "cache = {}\n",
    "def prompt_hash(prompt):\n",
    "    return hashlib.sha256(prompt.encode()).hexdigest()\n",
    "def fetch_answer(prompt):\n",
    "    key = prompt_hash(prompt)\n",
    "    if key in cache:\n",
    "        print(\"Cache hit! Returning cached response.\")\n",
    "        return cache[key]\n",
    "    print(\"Cache miss. Calling API...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    cache[key] = result\n",
    "    return result\n",
    "\n",
    "# Test the caching functionality\n",
    "test_prompt = \"What is the capital of France?\"\n",
    "\n",
    "print(\"First call:\")\n",
    "answer1 = fetch_answer(test_prompt)\n",
    "print(f\"Answer: {answer1}\\n\")\n",
    "\n",
    "print(\"Second call (should use cache):\")\n",
    "answer2 = fetch_answer(test_prompt)\n",
    "print(f\"Answer: {answer2}\\n\")\n",
    "\n",
    "print(f\"Cache size: {len(cache)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b96fcd",
   "metadata": {},
   "source": [
    "## Example 4: Batch/Parallel Requests\n",
    "Process multiple API requests concurrently to reduce total execution time. This example compares sequential vs parallel processing and shows the performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "270de584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 5 API calls...\n",
      "\n",
      "--- Sequential Processing ---\n",
      "Sequential time: 7.10 seconds\n",
      "\n",
      "--- Parallel Processing ---\n",
      "Sequential time: 7.10 seconds\n",
      "\n",
      "--- Parallel Processing ---\n",
      "Parallel time: 1.07 seconds\n",
      "\n",
      "Speedup: 6.66x faster\n",
      "\n",
      "--- Results ---\n",
      "1. Q: Explain machine learning in one sentence.\n",
      "   A: Machine learning is a subset of artificial intelligence that allows computers to learn and improve from experience without being explicitly programmed.\n",
      "\n",
      "2. Q: What is the capital of Japan?\n",
      "   A: The capital of Japan is Tokyo.\n",
      "\n",
      "3. Q: Define artificial intelligence briefly.\n",
      "   A: Artificial intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. This includes tasks such as reasoning, learning, problem-solving, perception, and language understanding.\n",
      "\n",
      "4. Q: How does photosynthesis work in simple terms?\n",
      "   A: Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight into energy in the form of glucose (sugar). This process takes place in the chloroplasts of plant cells.\n",
      "\n",
      "During photosynthesis, plants take in carbon dioxide from\n",
      "\n",
      "5. Q: What is the speed of light?\n",
      "   A: The speed of light in a vacuum is approximately 299,792,458 meters per second (or about 186,282 miles per second).\n",
      "\n",
      "Parallel time: 1.07 seconds\n",
      "\n",
      "Speedup: 6.66x faster\n",
      "\n",
      "--- Results ---\n",
      "1. Q: Explain machine learning in one sentence.\n",
      "   A: Machine learning is a subset of artificial intelligence that allows computers to learn and improve from experience without being explicitly programmed.\n",
      "\n",
      "2. Q: What is the capital of Japan?\n",
      "   A: The capital of Japan is Tokyo.\n",
      "\n",
      "3. Q: Define artificial intelligence briefly.\n",
      "   A: Artificial intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. This includes tasks such as reasoning, learning, problem-solving, perception, and language understanding.\n",
      "\n",
      "4. Q: How does photosynthesis work in simple terms?\n",
      "   A: Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight into energy in the form of glucose (sugar). This process takes place in the chloroplasts of plant cells.\n",
      "\n",
      "During photosynthesis, plants take in carbon dioxide from\n",
      "\n",
      "5. Q: What is the speed of light?\n",
      "   A: The speed of light in a vacuum is approximately 299,792,458 meters per second (or about 186,282 miles per second).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Create async client\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "# Sample prompts to process in parallel\n",
    "prompts = [\n",
    "    \"Explain machine learning in one sentence.\",\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Define artificial intelligence briefly.\",\n",
    "    \"How does photosynthesis work in simple terms?\",\n",
    "    \"What is the speed of light?\"\n",
    "]\n",
    "\n",
    "async def fetch_completion(prompt):\n",
    "    \"\"\"Fetch a single completion asynchronously\"\"\"\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def parallel_requests(prompts):\n",
    "    \"\"\"Process multiple prompts in parallel\"\"\"\n",
    "    tasks = [fetch_completion(prompt) for prompt in prompts]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "def sequential_requests(prompts):\n",
    "    \"\"\"Process prompts sequentially for comparison\"\"\"\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        results.append(response.choices[0].message.content)\n",
    "    return results\n",
    "\n",
    "# Compare sequential vs parallel processing\n",
    "print(\"Testing 5 API calls...\")\n",
    "print(\"\\n--- Sequential Processing ---\")\n",
    "start_time = time.time()\n",
    "sequential_results = sequential_requests(prompts)\n",
    "sequential_time = time.time() - start_time\n",
    "print(f\"Sequential time: {sequential_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n--- Parallel Processing ---\")\n",
    "start_time = time.time()\n",
    "parallel_results = await parallel_requests(prompts)\n",
    "parallel_time = time.time() - start_time\n",
    "print(f\"Parallel time: {parallel_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nSpeedup: {sequential_time/parallel_time:.2f}x faster\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n--- Results ---\")\n",
    "for i, (prompt, result) in enumerate(zip(prompts, parallel_results)):\n",
    "    print(f\"{i+1}. Q: {prompt}\")\n",
    "    print(f\"   A: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f4c14",
   "metadata": {},
   "source": [
    "## Example 5: Streaming Responses\n",
    "Reduce perceived latency by streaming the response in real-time instead of waiting for the complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a28be984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Non-streaming Response ---\n",
      "Quantum computing is a type of computing that uses quantum bits, or 'qubits', instead of the typical bits used in digital computing. While regular bits can be either 0s or 1s, qubits can be both at the same time, thanks to a property called superposition. This allows quantum computers to process a much higher number of possibilities instantly. \n",
      "\n",
      "In addition to superposition, quantum computers also use another quantum mechanic property called entanglement, where the state of one particle is directly related to the state of another, no matter the distance between them. This allows quantum computers to process information in a way that is significantly faster and more complex than traditional computers. \n",
      "\n",
      "However, quantum computing is currently still in the experimental stage, as there are many difficulties in maintaining the state of qubits and reducing errors in calculations.\n",
      "\n",
      "Total time to first output: 9.16 seconds\n",
      "\n",
      "\n",
      "--- Streaming Response ---\n",
      "Quantum computing is a type of computing that uses quantum bits, or 'qubits', instead of the typical bits used in digital computing. While regular bits can be either 0s or 1s, qubits can be both at the same time, thanks to a property called superposition. This allows quantum computers to process a much higher number of possibilities instantly. \n",
      "\n",
      "In addition to superposition, quantum computers also use another quantum mechanic property called entanglement, where the state of one particle is directly related to the state of another, no matter the distance between them. This allows quantum computers to process information in a way that is significantly faster and more complex than traditional computers. \n",
      "\n",
      "However, quantum computing is currently still in the experimental stage, as there are many difficulties in maintaining the state of qubits and reducing errors in calculations.\n",
      "\n",
      "Total time to first output: 9.16 seconds\n",
      "\n",
      "\n",
      "--- Streaming Response ---\n",
      "QuantumQuantum computing is computing is a type a type of computing technology that of computing technology that uses the uses the principles of principles of quantum mechanics quantum mechanics. Unlike classical computers which use bits (0s or . Unlike classical computers which use bits (0s or 1s1s) as) as their smallest their smallest unit of unit of information, information, quantum computers quantum computers use quantum use quantum bits, bits, or q or qubits.ubits. \n",
      "\n",
      "A \n",
      "\n",
      "A qubit qubit can be can be both  both 0 and0 and 1 1 at the at the same time same time, thanks to a property called superposition. This means quantum, thanks to a property called superposition. This means quantum computers can computers can process a process a lot more lot more information and information and much faster much faster than traditional than traditional computers. computers. \n",
      "\n",
      "Another \n",
      "\n",
      "Another key principle key principle is ent is entanglementanglement, where qubits become, where qubits become interconnected interconnected and the and the state of state of one can one can instantly affect instantly affect the state the state of the of the other, other, no matter no matter the distance the distance. This. This helps in helps in faster data faster data processing.\n",
      "\n",
      " processing.\n",
      "\n",
      "These characteristicsThese characteristics make quantum make quantum computers powerful computers powerful for certain for certain tasks like tasks like factoring large numbers factoring large numbers, sim, simulating complexulating complex chemical processes chemical processes, and, and optimizing large optimizing large systems which systems which would be would be very challenging very challenging for classical for classical computers. computers. However, However, it's it's still an still an emerging technology emerging technology with many with many practical challenges practical challenges to overcome. to overcome.\n",
      "\n",
      "Time to first chunk: 0.79 seconds\n",
      "Perceived latency improvement: 91.3%\n",
      "\n",
      "\n",
      "Time to first chunk: 0.79 seconds\n",
      "Perceived latency improvement: 91.3%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"--- Non-streaming Response ---\")\n",
    "start_time = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing simply.\"}],\n",
    "    temperature=0.7,\n",
    "    stream=False\n",
    ")\n",
    "non_streaming_time = time.time() - start_time\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTotal time to first output: {non_streaming_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n\\n--- Streaming Response ---\")\n",
    "start_time = time.time()\n",
    "first_chunk_time = None\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing simply.\"}],\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        if first_chunk_time is None:\n",
    "            first_chunk_time = time.time() - start_time\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(f\"\\n\\nTime to first chunk: {first_chunk_time:.2f} seconds\")\n",
    "print(f\"Perceived latency improvement: {((non_streaming_time - first_chunk_time) / non_streaming_time * 100):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
