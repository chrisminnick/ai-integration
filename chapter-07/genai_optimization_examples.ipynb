{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da3be7e",
   "metadata": {},
   "source": [
    "# Generative AI Optimization Examples\n",
    "This notebook demonstrates several prompt optimization techniques using the OpenAI API. You'll need an API key to run these examples.\n",
    "\n",
    "> ⚠️ Make sure you have installed the `openai` Python package and set your `OPENAI_API_KEY` environment variable or configure it manually in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ad1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "import os\n",
    "\n",
    "# Optionally set your API key here if not using environment variables\n",
    "# openai.api_key = 'your-api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12375e88",
   "metadata": {},
   "source": [
    "## Example 1: Prompt Compression\n",
    "Reducing unnecessary instructions from the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1ec79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To reduce the latency of your GPT-4 application in production, consider the following steps:\n",
      "\n",
      "1. Optimize the Model: Use quantization or pruning to reduce the size of the model. This not only reduces memory usage but also speeds up model inference.\n",
      "\n",
      "2. Use GPUs: GPUs are designed to handle massive amounts of data in parallel, which can significantly reduce the latency during model inference.\n",
      "\n",
      "3. Optimize Batch Size: Adjusting the batch size depending on your system's specification might help in reducing latency. Be mindful that a larger batch size can maximize computational efficiency but may also increase latency.\n",
      "\n",
      "4. Use Caching: Cache the responses for repeated queries. If certain requests are made often, caching the responses can significantly reduce latency.\n",
      "\n",
      "5. Optimize Pre-processing and Post-processing: Ensure the data preprocessing and post-processing steps are as efficient as they can be. \n",
      "\n",
      "6. Use Edge Computing: If the application is latency-sensitive and the model isn’t too large, consider edge computing. This means running the model on the client device or on a server close to the client.\n",
      "\n",
      "7. Parallel Processing: If the application needs to process multiple requests simultaneously, consider parallel processing.\n",
      "\n",
      "Remember, the steps suitable for your application depend on the specific requirements and constraints of your project.\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 52\n",
      "Completion tokens: 255\n",
      "Total tokens: 307\n"
     ]
    }
   ],
   "source": [
    "# Verbose prompt\n",
    "verbose_prompt = '''\n",
    "You're an intelligent and helpful assistant. Please help me answer this question in a clear, concise, and professional manner.\n",
    "The question is: How can I reduce the latency of my GPT-4 application in production?\n",
    "'''\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-4\",\n",
    "messages=[{\"role\": \"user\", \"content\": verbose_prompt}],\n",
    "temperature=0.7)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f600c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of the time of writing this, GPT-4 has not been released by OpenAI. Therefore, it's hard to provide specific advice on how to reduce its latency in production. However, the following general tips might be useful for optimizing AI models like GPT-3 and potentially GPT-4:\n",
      "\n",
      "1. Model Optimization: Use techniques like quantization, pruning, and knowledge distillation to reduce the complexity of the model. This could potentially reduce the time it takes to make predictions.\n",
      "\n",
      "2. Hardware Acceleration: Use GPUs or other hardware accelerators, which are often designed to efficiently run AI workloads.\n",
      "\n",
      "3. Efficient Coding: Optimize your code to reduce unnecessary computations. Make sure your implementation is as efficient as possible.\n",
      "\n",
      "4. Use Edge Computing: If possible, deploy the model closer to the user to reduce network latency. This could mean using edge computing solutions.\n",
      "\n",
      "5. Parallel Processing: If the model needs to make multiple predictions, try to parallelize these predictions to speed up the overall process.\n",
      "\n",
      "6. Use Efficient Data Structures: The way you store and access your data can have a big impact on performance. Make sure you're using the most efficient data structures for your specific use case.\n",
      "\n",
      "7. Model Pipelining: If the model is part of a larger pipeline, make sure the entire pipeline is as efficient as possible. This could mean optimizing the way data is passed between different stages of the pipeline.\n",
      "\n",
      "8. Caching: If the model needs to make the same prediction multiple times, consider caching the result to avoid unnecessary computation.\n",
      "\n",
      "Remember, it's always important to measure the performance of your model and the system it's running on to identify bottlenecks and validate whether your optimizations are having the desired effect.\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 19\n",
      "Completion tokens: 350\n",
      "Total tokens: 369\n"
     ]
    }
   ],
   "source": [
    "# Compressed prompt\n",
    "compressed_prompt = \"How can I reduce GPT-4 latency in production?\"\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-4\",\n",
    "messages=[{\"role\": \"user\", \"content\": compressed_prompt}],\n",
    "temperature=0.7)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1a4fa",
   "metadata": {},
   "source": [
    "## Example 2: Output Compression\n",
    "Limit the size and verbosity of the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e0db0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy, usually from the sun, into chemical energy stored in glucose molecules. This process is essential for the survival of plants, as it is how they produce the food they need to grow and carry out their metabolic functions.\n",
      "\n",
      "During photosynthesis, plants use carbon dioxide from the air and water from the soil to produce glucose and oxygen. This process takes place in the chloroplasts of plant cells, which contain the pigment chlorophyll that absorbs light energy. The light energy is used to split water molecules into oxygen and hydrogen ions, and the hydrogen ions are then used to convert carbon dioxide into glucose.\n",
      "\n",
      "Overall, the equation for photosynthesis is:\n",
      "6CO2 + 6H2O + light energy → C6H12O6 + 6O2\n",
      "\n",
      "In addition to providing plants with the energy they need to grow and reproduce, photosynthesis also plays a critical role in the global carbon cycle by removing carbon dioxide from the atmosphere and releasing oxygen back into it. This process is essential for maintaining the balance of gases in the Earth's atmosphere and supporting life on the planet.\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 13\n",
      "Completion tokens: 234\n",
      "Total tokens: 247\n"
     ]
    }
   ],
   "source": [
    "# Unconstrained response\n",
    "prompt = \"Tell me about photosynthesis.\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f860abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Process by which plants and other organisms convert sunlight into energy\n",
      "- Involves absorption of light by chlorophyll in plant cells\n",
      "- Produces oxygen as a byproduct\n",
      "\n",
      "--- Token Usage ---\n",
      "Prompt tokens: 25\n",
      "Completion tokens: 36\n",
      "Total tokens: 61\n"
     ]
    }
   ],
   "source": [
    "# Constrained response\n",
    "prompt = \"Tell me about photosynthesis.\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Answer in 3 short bullet points.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "],\n",
    "max_tokens=100)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n--- Token Usage ---\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
